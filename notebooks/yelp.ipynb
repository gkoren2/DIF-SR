{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "difsr_root= os.path.dirname(os.getcwd())\n",
    "sys.path.insert(1, difsr_root)\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset\n",
    "from recbole.data.utils import get_dataloader\n",
    "from recbole.utils import init_logger, init_seed, get_model, get_trainer, set_color\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = os.path.abspath('../dataset')\n",
    "# os.listdir(dataset_root)\n",
    "os.listdir('../dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    model:str = 'SASRecDX2'\n",
    "    dataset:str = 'yelp'\n",
    "    config_files:str = None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=os.path.join(dataset_root,'yelp')\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset using recbole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset through the recbole framework\n",
    "args=Arguments(dataset=\"yelp\",config_files=os.path.join(difsr_root,'configs/yelp_cat_L1.yaml'))\n",
    "config_file_list = args.config_files.strip().split(' ') if args.config_files else None\n",
    "config = Config(model=args.model, dataset=f'{args.dataset}', config_file_list=config_file_list)\n",
    "config.final_config_dict['data_path'] = os.path.join(difsr_root,config.final_config_dict['data_path'])\n",
    "config.final_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(config)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.item_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the list of columns that can be considered as features\n",
    "dataset.field2token_id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.item_feat['categories'].loc[0].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read item data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf=pd.read_csv(os.path.join(dataset_path,'yelp.item'),sep='\\t')\n",
    "ydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many non-Nan values are there in each column\n",
    "ydf.count()/len(ydf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read original data\n",
    "reading the data from the Yelp dataset, before it has been preprocessed by recbole. the dataset was downloaded from Kaggle (version 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "yelp_orig_path='/home/guy/sd1tb/datasets/yelp'\n",
    "os.listdir(yelp_orig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json file to dataframe\n",
    "def read_json_to_df(path):\n",
    "    with open(path) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "yelp_review=read_json_to_df(os.path.join(yelp_orig_path,'yelp_academic_dataset_review.json'))\n",
    "yelp_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by reviews of user u by day of review and count the number of reviews\n",
    "u.groupby(u['date'].dt.date).count()['review_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many users have more than 100 reviews\n",
    "len(yelp_review.groupby('user_id').count()['review_id'][yelp_review.groupby('user_id').count()['review_id']>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review['user_id'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset through the recbole framework\n",
    "# args=Arguments(dataset=\"yelp\",config_files=os.path.join(difsr_root,'configs/yelp_cat_L1.yaml'))\n",
    "# args=Arguments(dataset=\"yelp\",config_files=os.path.join(difsr_root,'configs/yelp_cat_city_L1.yaml'))\n",
    "args=Arguments(dataset=\"yelp\",config_files=os.path.join(difsr_root,'configs/exp4/yelp_cat_city.yaml'))\n",
    "config_file_list = args.config_files.strip().split(' ') if args.config_files else None\n",
    "config = Config(model=args.model, dataset=f'{args.dataset}', config_file_list=config_file_list)\n",
    "config.final_config_dict['data_path'] = os.path.join(difsr_root,config.final_config_dict['data_path'])\n",
    "config.final_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_ID = config['ITEM_ID_FIELD']\n",
    "ITEM_SEQ = ITEM_ID + config['LIST_SUFFIX']\n",
    "ITEM_SEQ_LEN = config['ITEM_LIST_LENGTH_FIELD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['attribute_predictor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of type SequentialDataset\n",
    "dataset = create_dataset(config)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_datasets = dataset.build()\n",
    "train_dataset, valid_dataset, test_dataset = built_datasets\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of the model\n",
    "model = get_model(config['model'])(config, dataset).to(config['device'])\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_blocks = list(dict(model.trm_encoder.layer.named_children()).values())\n",
    "# print(len(attn_blocks))\n",
    "# attn_blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model path\n",
    "model_path=os.path.join(difsr_root,'saved/yelp_cat_city')\n",
    "os.listdir(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name=os.path.join(model_path,'SASRecD-Apr-09-2023_e150.pth')\n",
    "\n",
    "# load the model\n",
    "model_file = torch.load(model_file_name)\n",
    "model.load_state_dict(model_file['state_dict'])\n",
    "model.load_other_parameter(model_file.get('other_parameter'))\n",
    "print('Loading model structure and parameters from {}'.format(model_file))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=13\n",
    "n_inter=1 # number of interactions to sample\n",
    "interaction = test_dataset[idx:idx+n_inter]\n",
    "pos_i = interaction[ITEM_ID]\n",
    "interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction.categories\n",
    "print(interaction.review_id_list)\n",
    "print(interaction.business_id_list)\n",
    "print(interaction.stars_list)\n",
    "print(interaction.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the embedding of the predicted item\n",
    "item_seq = interaction[ITEM_SEQ].to(config['device'])\n",
    "item_seq_len = interaction[ITEM_SEQ_LEN].to(config['device'])\n",
    "gt_item = interaction[ITEM_ID]\n",
    "item_embeddings = model.item_embedding.weight\n",
    "seq_output=model(item_seq, item_seq_len)\n",
    "scores = torch.matmul(seq_output, item_embeddings.transpose(0, 1))  # [B, item_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.full_sort_predict(interaction)\n",
    "topk=20\n",
    "topk_scores, topk_items = torch.topk(scores, topk)\n",
    "print(topk_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret prediction\n",
    "following the method from Hila's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug - running all the necessary cells if not run as part of the Manual prediction\n",
    "built_datasets = dataset.build()\n",
    "train_dataset, valid_dataset, test_dataset = built_datasets\n",
    "model = get_model(config['model'])(config, dataset).to(config['device'])\n",
    "\n",
    "model_path=os.path.join(difsr_root,'saved/yelp_cat_city')\n",
    "model_file_name=os.path.join(model_path,'SASRecD-Apr-09-2023_e150.pth')\n",
    "\n",
    "# load the model\n",
    "model_file = torch.load(model_file_name)\n",
    "model.load_state_dict(model_file['state_dict'])\n",
    "model.load_other_parameter(model_file.get('other_parameter'))\n",
    "print('Loading model structure and parameters from {}'.format(model_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=16\n",
    "n_inter=3 # number of interactions to sample\n",
    "interaction = test_dataset[idx:idx+n_inter]\n",
    "# interaction = train_dataset[idx:idx+n_inter]\n",
    "\n",
    "pos_i = interaction[ITEM_ID]\n",
    "print(interaction[ITEM_SEQ_LEN])\n",
    "print(pos_i)\n",
    "interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, expl = model.interpret(interaction,start_layer=0)\n",
    "expl.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is to debug the gradients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def check_grads(interaction,layer=0, index=None):\n",
    "        device = model.device\n",
    "        item_seq = interaction[model.ITEM_SEQ].to(device)\n",
    "        batch_size = interaction.length\n",
    "        item_seq_len = interaction[model.ITEM_SEQ_LEN].to(device)\n",
    "        seq_output = model(item_seq, item_seq_len)\n",
    "        item_embeddings = model.item_embedding.weight\n",
    "        scores = torch.matmul(seq_output, item_embeddings.transpose(0, 1))  # [B, item_num]\n",
    "        scores = scores.softmax(dim=-1)\n",
    "        if index is None:\n",
    "            index = scores.argmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "        one_hot = np.zeros((scores.shape[0], scores.shape[1]), dtype=np.float32)\n",
    "        one_hot[torch.arange(scores.shape[0]), index] = 1\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "        one_hot = torch.sum(one_hot.cuda(device) * scores,dim=-1)\n",
    "        model.zero_grad()        # why do we need it ? \n",
    "\n",
    "        attn_blocks = list(dict(model.trm_encoder.layer.named_children()).values())\n",
    "        blk=attn_blocks[layer]\n",
    "        S=interaction[model.ITEM_SEQ_LEN]\n",
    "        result={}\n",
    "        for i in range(batch_size):\n",
    "            grad = torch.autograd.grad(one_hot[i], [blk.multi_head_attention.attention_probs], retain_graph=True)[0].detach()\n",
    "            neg_grad = (grad<0).sum(dim=1)[:,S[i]-1].cpu()\n",
    "            result.update({S[i]:neg_grad[i]})\n",
    "\n",
    "        return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_grads(interaction,layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=10\n",
    "res=dict()\n",
    "for b in tqdm(range(len(test_dataset)//bs)):\n",
    "    interactions = test_dataset[b*bs:b*bs+bs]\n",
    "    res.update(check_grads(interactions,layer=-1))\n",
    "\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view attention maps\n",
    "view attention maps from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask\n",
    "attn_mask = model.extended_attention_mask.detach().cpu().squeeze(0)  # [1,L,L]\n",
    "\n",
    "# accessing the attributes attention scores of the first layer [b,h,L,p,L] where p is the number of attributes\n",
    "attr_attn_scores = model.trm_encoder.layer[0].multi_head_attention.attribute_attention_table.detach().cpu().permute(0,3,1,2,4).squeeze(0)    #[p,h,L,L]\n",
    "\n",
    "# accessing the item ID attention scores of the first layer: [b,h,L,L]\n",
    "item_attn_scores = model.trm_encoder.layer[0].multi_head_attention.item_attention_scores.detach().cpu().squeeze(0)     #[h,L,L]\n",
    "\n",
    "# accessing the position attention scores of the first layer: [b,h,L,L]\n",
    "pos_attn_scores = model.trm_encoder.layer[0].multi_head_attention.pos_scores.detach().cpu().squeeze(0)    #[h,L,L]\n",
    "\n",
    "# attention probs - after fusion of attributes, score and item ID attention and doing softmax normalization\n",
    "attn_probs = model.trm_encoder.layer[0].multi_head_attention.attention_probs.detach().cpu().squeeze(0)   #[h,L,L]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask.shape, attr_attn_scores.shape, item_attn_scores.shape, pos_attn_scores.shape, attn_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_view(attention_weights,attention_mask,item_seq_len,norm_to_probs=True):\n",
    "    # Reshape attention weights tensor to have shape (num_heads, seq_length, seq_length)\n",
    "    if norm_to_probs:\n",
    "        attention_weights = attention_weights + attention_mask\n",
    "        attention_weights = torch.nn.Softmax(dim=-1)(attention_weights)\n",
    "    attention_weights = attention_weights.numpy()[:,:item_seq_len,:item_seq_len]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    attention_weights = np.transpose(attention_weights, (1, 2, 0))  # switch to (seq_length, seq_length, num_heads)\n",
    "\n",
    "    # Normalize each head's attention weights across all tokens in sequence\n",
    "    for h in range(num_heads):\n",
    "        attention_weights[:, :, h] = attention_weights[:, :, h] / np.sum(attention_weights[:, :, h], axis=1, keepdims=True)\n",
    "\n",
    "    # Visualize attention maps as heatmaps or matrices\n",
    "    fig, axs = plt.subplots(nrows=num_heads, ncols=1, figsize=(10, 20))\n",
    "    for h in range(num_heads):\n",
    "        im=axs[h].imshow(attention_weights[:, :, h], cmap='viridis', interpolation='nearest')\n",
    "        axs[h].set_title('Head {}'.format(h+1))\n",
    "        # add colorbar to each plot\n",
    "        fig.colorbar(im, ax=axs[h])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_view(item_attn_scores,attn_mask,item_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_view(attr_attn_scores[1],attn_mask,item_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_view(pos_attn_scores,attn_mask,item_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_view(attn_probs,attn_mask,item_seq_len,norm_to_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_interaction(interaction_idx, model, test_dataset, topk=100):\n",
    "    interaction = test_dataset[interaction_idx:interaction_idx+1]\n",
    "    item_seq = interaction[ITEM_SEQ].to(config['device'])\n",
    "    item_seq_len = interaction[ITEM_SEQ_LEN].to(config['device'])\n",
    "    gt_item = interaction[ITEM_ID]\n",
    "    item_embeddings = model.item_embedding.weight\n",
    "    seq_output=model(item_seq, item_seq_len)\n",
    "    scores = torch.matmul(seq_output, item_embeddings.transpose(0, 1))  # [B, item_num]\n",
    "    topk_scores, topk_items = torch.topk(scores, topk)\n",
    "    \n",
    "    return gt_item.detach().cpu().numpy(), topk_items.detach().cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_item, topk_items = analyze_interaction(13, model, test_dataset, topk=20)\n",
    "gt_item in topk_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[gt in topk for gt, topk in [analyze_interaction(i, model, test_dataset, topk=10) for i in range(len(test_dataset))]]\n",
    "# this should be equivalent to the recall@topk \n",
    "sum(result)/len(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.data import data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction=test_data.dataset[13:14]\n",
    "interaction.business_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction, scores, positive_u, positive_i = eval_func(batched_data)\n",
    "batched_data = next(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data[0].business_id_list[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of the model\n",
    "model = get_model(config['model'])(config, dataset).to(config['device'])\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model path\n",
    "model_path=os.path.join(difsr_root,'saved/yelp_cat_L1')\n",
    "os.listdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trainer\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name=os.path.join(model_path,'SASRecD-Apr-09-2023_e150.pth')\n",
    "test_result = trainer.evaluate(test_data,\n",
    "                               load_best_model=True,\n",
    "                               model_file = model_file_name,\n",
    "                               show_progress=config['show_progress'])\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name=os.path.join(model_path,'SASRecD-Apr-09-2023_20-03-18.pth')\n",
    "test_result = trainer.evaluate(test_data,\n",
    "                               load_best_model=True,\n",
    "                               model_file = model_file_name,\n",
    "                               show_progress=config['show_progress'])\n",
    "test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
